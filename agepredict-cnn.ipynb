{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Age Prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: \n",
    "To predict a person's age using a CNN model that extracts facial features from images.\n",
    "\n",
    "**Dataset**:\n",
    "I will be using the [UTKFace](https://www.kaggle.com/datasets/jangedoo/utkface-new) dataset, which contains labeled facial images for age prediction.\n",
    "\n",
    "**Approach**:\n",
    "To tackle this problem, I have two options:\n",
    "- Using a Pretrained Model (ResNet18):\n",
    "    - I plan to reuse the ResNet18 architecture (a residual network with 18 layers, introduced in 2015). I will modify the final fully connected layer to suit a regression task, enabling the model to predict continuous age values.\n",
    "- Building a Custom CNN from Scratch:\n",
    "    - Alternatively, I may design and train a custom CNN tailored specifically for this task, focusing on optimizing the architecture for age prediction.\n",
    "\n",
    "Thank you for your interest in my project!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "<ul>\n",
    "    <li><a href=\"#1-import-data\">1. Import Data</a></li>\n",
    "    <li><a href=\"#2-building-and-training\">2. Building And Training</a>\n",
    "        <ul>\n",
    "            <li><a href=\"#21-using-a-pretrained-model-resnet18\">2.1 Using a Pretrained Model (ResNet18)</a></li>\n",
    "            <li><a href=\"#22-building-a-custom-cnn-from-scratch\">2.2 Building a Custom CNN from Scratch</a></li>\n",
    "            <li><a href=\"#23-model-comparison\">2.3 Model Comparison</a></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T01:10:37.255912Z",
     "iopub.status.busy": "2025-04-15T01:10:37.255566Z",
     "iopub.status.idle": "2025-04-15T01:10:37.261223Z",
     "shell.execute_reply": "2025-04-15T01:10:37.260359Z",
     "shell.execute_reply.started": "2025-04-15T01:10:37.255892Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "\n",
    "from torchvision.models import resnet18\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Import Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"jangedoo/utkface-new\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T01:10:41.125252Z",
     "iopub.status.busy": "2025-04-15T01:10:41.124969Z",
     "iopub.status.idle": "2025-04-15T01:10:41.183961Z",
     "shell.execute_reply": "2025-04-15T01:10:41.183368Z",
     "shell.execute_reply.started": "2025-04-15T01:10:41.125230Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class ReadDataset(Dataset):\n",
    "    def __init__(self, root, train = True, transform=None):\n",
    "        self.path = os.path.join(root, \"UTKFace\" if train else \"crop_part1\")\n",
    "        self.images, self.labels = [], [] \n",
    "        \n",
    "        # Define the default transform if none provided\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),  # c,w,h -> h,w,c\n",
    "            transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)  # Scale [-1, 1]\n",
    "        ])\n",
    "        \n",
    "        # Read the image paths and labels\n",
    "        for file_name in os.listdir(self.path):\n",
    "            file_path = os.path.join(self.path, file_name)\n",
    "            self.images.append(file_path)\n",
    "            \n",
    "            # Get age for label (assuming file_name format is like \"age-gender-otherinfo.jpg\")\n",
    "            age = int(file_name.split(\"_\")[0])\n",
    "            self.labels.append(float(age))  # Label is the age\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # Load image using cv2 (in BGR format)\n",
    "        img_path = self.images[index]\n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        # Convert BGR to RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Convert image to PIL Image to apply transforms\n",
    "        img = Image.fromarray(img)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)  # Apply transformations\n",
    "        \n",
    "        # Get the label as a tensor\n",
    "        label = torch.tensor(self.labels[index], dtype=torch.float)\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "root = path\n",
    "# get dataset\n",
    "train_dataset = ReadDataset(root)\n",
    "test_dataset = ReadDataset(root, train = False)\n",
    "train_loader = DataLoader(train_dataset, batch_size= 64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size= 64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image,label = train_dataset.__getitem__(10)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def show_batch(images, labels):\n",
    "    index = int(np.random.random_integers(0, 64))\n",
    "    # Un-normalize \n",
    "    images = images[index] * 0.5 + 0.5  #  [-1, 1] → [0, 1]\n",
    "\n",
    "    # Hiển thị\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.imshow(images.permute(1, 2, 0))  # [C, H, W] → [H, W, C]\n",
    "    if labels is not None:\n",
    "        plt.title(f\"Labels: {labels[index]}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# get the first batch\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# Hiển thị batch\n",
    "show_batch(images,labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Building And Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1 Using a Pretrained Model (ResNet18)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's see ResNet18's Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_pretrain = resnet18(pretrained=True)\n",
    "print(resnet_pretrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adapting this Model to be suitable of this Problem accordingly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class adapt_resnet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        old_model = resnet18(pretrained=True)\n",
    "        self.features =  nn.Sequential(\n",
    "            old_model.conv1,\n",
    "            old_model.bn1,\n",
    "            old_model.relu,\n",
    "            old_model.layer1,\n",
    "            old_model.layer2,\n",
    "            old_model.layer3,\n",
    "            nn.AdaptiveAvgPool2d((1, 1)), \n",
    "        )\n",
    "        \n",
    "        # add Dropout Layers to avoid overfitting\n",
    "        self.regression = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256,1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "         x = self.features(input)\n",
    "         x = torch.flatten(x, 1)  # flatten \n",
    "         x = self.regression(x)\n",
    "         return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if train_on_gpu:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model_v1 = adapt_resnet18()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = optim.Adam(model_v1.parameters(), lr=1e-2, weight_decay=1e-5)\n",
    "\n",
    "# loss function for regression\n",
    "criterion = nn.MSELoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Directory to save model\n",
    "if not os.path.exists('Models'):\n",
    "    os.mkdir('Models')\n",
    "    print(\"Directory 'Models' has been created.\")\n",
    "else:\n",
    "    print(\"Directory 'Models' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model including number of epoch, state_dict, optimizer and loss\n",
    "def save_checkpoint(state, filename=\"checkpoint.pth.tar\"):\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the best model when training\n",
    "def train_validate_and_save_best_mode(model, criterion, optimizer, epochs, file_name):\n",
    "\n",
    "    train_loss_list, test_loss_list = [], []\n",
    "    model.to(device)\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(1, epochs+1):\n",
    "\n",
    "        # keep track of training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        train_loader_bar = tqdm(train_loader, desc = \"Trainning\", leave=False) # illustrate process\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader_bar):\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            target = target.float()\n",
    "            target = target.view(-1, 1)\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "\n",
    "            train_loader_bar.set_postfix(loss = loss.item())\n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        \n",
    "        test_loader_bar = tqdm(test_loader, desc = \"Validate\", leave=False)\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(test_loader_bar):\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(data)\n",
    "                # calculate the batch loss\n",
    "                target = target.float()\n",
    "                target = target.view(-1, 1)\n",
    "                loss = criterion(output, target)\n",
    "                # update average validation loss \n",
    "                valid_loss += loss.item()*data.size(0)\n",
    "                test_loader_bar.set_postfix(loss = valid_loss)\n",
    "        \n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(train_loader.sampler)\n",
    "        valid_loss = valid_loss/len(test_loader.sampler)\n",
    "\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'loss': valid_loss\n",
    "            }, filename=(\"Models/{}.pth.tar\".format(file_name)))\n",
    "\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, train_loss, valid_loss))\n",
    "        \n",
    "        train_loss_list.append(train_loss)\n",
    "        test_loss_list.append(valid_loss)\n",
    "    \n",
    "    return train_loss_list, test_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualization for training and validating \n",
    "def evaluate(nb_epoch, train_loss_list, test_loss_list):\n",
    "    epoch_list = [i for i in range(1, nb_epoch+1)]\n",
    "    plt.plot(epoch_list,train_loss_list, marker = \"o\" , color = \"blue\")\n",
    "    plt.plot(epoch_list,test_loss_list, marker = \"o\" , color = \"red\")\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training\n",
    "epochs = 20 \n",
    "train_loss_list_v1, test_loss_list_v1 = train_validate_and_save_best_mode(model_v1,criterion, optimizer, epochs, \"Using_Pretrained_Model\")\n",
    "evaluate(epochs,train_loss_list_v1,test_loss_list_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 Building a Custom CNN from Scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AgeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)  # Output: (64, 55, 55)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), # (128, 54, 54)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)  # Output: (128, 27, 27)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1), # (128,26,26)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)  # Output: (128, 13, 13)\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=2), # 256, 14,14\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2) # 256, 7,7\n",
    "        )\n",
    "\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1 ),# 6 * 6 * 256 \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(12544, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)  # Output: age (regression)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)  # (3, 224, 224) → (64, 55, 55)\n",
    "        x = self.conv2(x)  # (64, 55, 55) → (128, 27, 27)\n",
    "        x = self.conv3(x)  # (128, 27, 27) → (128, 13, 13)\n",
    "        x = self.conv4(x)  # → (256, 13, 13)\n",
    "        x = self.conv5(x)  # → (256, 13, 13)\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten → (batch_size, 512*5*5)\n",
    "\n",
    "        x = self.dropout(x) \n",
    "        x = F.relu(self.fc1(x))  # → (512)\n",
    "        x = F.relu(self.fc2(x))  # → (128)\n",
    "        x = self.fc3(x)  # → (1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_v2 = AgeModel()\n",
    "\n",
    "print(model_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# we have to initialize one more time because these keep track the parameter of the previous Model\n",
    "\n",
    "# specify optimizer\n",
    "optimizer_v2 = optim.Adam(model_v2.parameters(), lr=1e-2, weight_decay=1e-5)\n",
    "\n",
    "# loss function for regression\n",
    "criterion_v2 = nn.MSELoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## training\n",
    "epochs = 20 \n",
    "train_loss_list_v2, test_loss_list_v2 = train_validate_and_save_best_mode(model_v2,criterion_v2, optimizer_v2, epochs, \"Custom_Model\")\n",
    "evaluate(epochs,train_loss_list_v2,test_loss_list_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3 Model Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compare(train_loss_list_v1, test_loss_list_v1, train_loss_list_v2, test_loss_list_v2):\n",
    "    \n",
    "    epochs = range(1, len(train_loss_list_v1) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot training loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss_list_v1, label='Model 1 - Train Loss', marker='o')\n",
    "    plt.plot(epochs, train_loss_list_v2, label='Model 2 - Train Loss', marker='x')\n",
    "    plt.title('Training Loss Comparison')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot test loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, test_loss_list_v1, label='Model 1 - Test Loss', marker='o')\n",
    "    plt.plot(epochs, test_loss_list_v2, label='Model 2 - Test Loss', marker='x')\n",
    "    plt.title('Test Loss Comparison')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "compare(train_loss_list_v1, test_loss_list_v1, train_loss_list_v2, test_loss_list_v2)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 44109,
     "sourceId": 78156,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "mle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
